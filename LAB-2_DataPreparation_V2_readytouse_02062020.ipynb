{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB-2 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c896d7b3255a1571e09ed89c2d2cfde62a73273"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Let's suppose that you are working for a bank (not so strange for some of you **:-)** ); this bank wants to offer new services to private individuals. Offered services include account management, loans, credit card services, etc. The bank is interesting in finding groups of clients as targets for the new services and the bank wants to set aside clients with risk of defaulting. Bank managers have a slight idea of who could be targeted (who to offer additional services) and who is not eligible (who to watch carefully to minimize the bank losses). The bank stores data about their clients:\n",
    "accounts (transactions within several months), loans already granted, credit cards issued... So that some idea of customer behavior can be extracted (and questions as well) by analyzing this data. \n",
    "\n",
    "We are going to follow the basic process of loan default prediction with machine learning algorithms.\n",
    "\n",
    "## Task description for LAB-2 and LAB-3\n",
    "\n",
    "We will work with 8 datasets extracted from the databases of the bank which, of course are stored on Db2 for z/OS (where else?). As data scientists, we will need to perform the following tasks:\n",
    "\n",
    "**LAB-2**\n",
    "* Use Python to connect to Db2 for z/OS and read tables into Pandas dataframes\n",
    "* Preprocess data for machine learning\n",
    "\n",
    "**LAB-3**\n",
    "* Train a ML model to predict customers who are more likely to default on loans\n",
    "* Evaluate model performance\n",
    "* Try to understand the key predictors of default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning of LAB-2: Analysis and data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this Jupyter notebook we will load data from some Db2 for z/OS tables and we will work the data contained in them. They are the same tables you used to visualize data in LAB-1. We need to reorder some tables, merge them, drop some columns and fill some gaps. After all this we will end up with a consistent Pandas dataframe suitable to train some machine learning models.\n",
    "\n",
    "**PLEASE:** Read with attention the instructions in the script you have in `*.pdf` format as well as the text cells in this labs. Our intention is that all the information needed to understand the lab can be found in these sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f46db8ee7a665d3744f4017bf0dd7b754c45352"
   },
   "source": [
    "# Exploratory Analysis\n",
    "\n",
    "To begin this exploratory analysis, we first import libraries. They are sets of Python programs and functions that will make our life easier. We are using some of the most popular packages used for data science with Python:\n",
    "\n",
    "* Pandas, used to manipulate data in tables called dataframes. \n",
    "* Numpy, used for numeric formatting, array management and numeric calculations.\n",
    "* Seaborn, used for fancy visualization inline with the code.\n",
    "* Matplotlib, used together with Seaborn.\n",
    "* Sklearn, contains the algorithms we will use for machine learning.\n",
    "\n",
    "With all these open source packages and Python tools we can do all the required works to arrange data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b00ea0d747ee508b5f2e37d640b27ee7f6615768",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.ticker as mtick\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "#import scikitplot as skplt\n",
    "plt.style.use('ggplot')\n",
    "#_______________________ Added to be able to load files from the Cloud Storage ________________________\n",
    "import dsx_core_utils, requests, os, io\n",
    "from dsx_core_utils import ProjectContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f65edc19aee51817b0e906524e3dfa1c4951027"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6db062645e2df8157ea9345517c4dc283f5f02b5"
   },
   "source": [
    "## Import and Update tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Db2 table `CARD` and make some small changes like formatting the date in the `issued` column and the card type into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL2\n",
    "\n",
    "#### Insert code to load CARD data in this cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 3\n",
    "\n",
    "# changes in two columns: issued and type\n",
    "card.issued = card.issued.str.strip(\"00:00:00\")\n",
    "card.type = card.type.map({\"gold\": 2, \"classic\": 1, \"junior\": 0})\n",
    "card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 4\n",
    "\n",
    "# Load table ACCOUNT\n",
    "\n",
    "# Add asset from remote connection\n",
    "account_raw = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info(pc, 'ACCOUNT')\n",
    "dataSource = dsx_core_utils.get_data_source_info(pc, dataSet['datasource'])\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = (dataSet['schema'] + '.' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table']\n",
    "account_raw = sparkSession.read.format(\"jdbc\").option(\"driver\",dataSource['driver_class']).option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dsx_core_utils.decrypt(dataSource['password'])).load()\n",
    "\n",
    "account = account_raw.toPandas()\n",
    "\n",
    "# Date is formatted applying a lambda function to the column\n",
    "account.date = account.date.apply(lambda x: pd.to_datetime(str(x), format=\"%y%m%d\"))\n",
    "account.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 5    \n",
    "    \n",
    "    # Load table DISP\n",
    "\n",
    "# Add asset from remote connection\n",
    "disp_raw = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info(pc, 'DISP')\n",
    "dataSource = dsx_core_utils.get_data_source_info(pc, dataSet['datasource'])\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = (dataSet['schema'] + '.' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table']\n",
    "disp_raw = sparkSession.read.format(\"jdbc\").option(\"driver\",dataSource['driver_class']).option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dsx_core_utils.decrypt(dataSource['password'])).load()\n",
    "\n",
    "disp = disp_raw.toPandas()\n",
    "\n",
    "# We select all rows where the value for type is OWNER then change the name of the column\n",
    "disp = disp[disp.type == \"OWNER\"]\n",
    "disp.rename(columns={\"type\": \"type_disp\"}, inplace=True)\n",
    "disp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 6\n",
    "\n",
    "# Load table CLIENT\n",
    "\n",
    "# Add asset from remote connection\n",
    "client_raw = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info(pc, 'CLIENT')\n",
    "dataSource = dsx_core_utils.get_data_source_info(pc, dataSet['datasource'])\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = (dataSet['schema'] + '.' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table']\n",
    "client_raw = sparkSession.read.format(\"jdbc\").option(\"driver\",dataSource['driver_class']).option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dsx_core_utils.decrypt(dataSource['password'])).load()\n",
    "\n",
    "client = client_raw.toPandas()\n",
    "\n",
    "# This is an interesting extraction of information which is coded in the birth date for customers:\n",
    "# the number is in the form YYMMDD for men,\n",
    "# the number is in the form YYMM+50DD for women\n",
    "# where YYMMDD is the date of birth\n",
    "# with the following code we extract the sex information from the birth dates and add a new \n",
    "# column with the sex: 0 for female, 1 for male.\n",
    "# Also we drop some columns: birth_number, month, year\n",
    "\n",
    "client[\"month\"] = client.birth_number.apply(\n",
    "    lambda x: x // 100 % 100, convert_dtype=True, args=())\n",
    "client[\"year\"] = client.birth_number.apply(\n",
    "    lambda x: x // 100 // 100, convert_dtype=True, args=())\n",
    "client[\"age\"] = 99 - client.year # age in 1999\n",
    "client[\"sex\"] = client.month.apply(lambda x: (x - 50) < 0, convert_dtype=True, args=()) \n",
    "client.sex = client.sex.astype(int)  # 0 for female, 1 for male\n",
    "client.drop([\"birth_number\", \"month\", \"year\"], axis=1, inplace=True)\n",
    "client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 7\n",
    "\n",
    "# Load table DISTRICT\n",
    "\n",
    "# Add asset from remote connection\n",
    "district_raw = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info(pc, 'DISTRICT')\n",
    "dataSource = dsx_core_utils.get_data_source_info(pc, dataSet['datasource'])\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = (dataSet['schema'] + '.' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table']\n",
    "district_raw = sparkSession.read.format(\"jdbc\").option(\"driver\",dataSource['driver_class']).option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dsx_core_utils.decrypt(dataSource['password'])).load()\n",
    "\n",
    "district = district_raw.toPandas()\n",
    "\n",
    "# Drop columns A2 and A3\n",
    "district.drop([\"A2\", \"A3\"], axis=1, inplace=True)\n",
    "district.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 8\n",
    "\n",
    "# Load table ORDER\n",
    "\n",
    "# Add asset from remote connection\n",
    "order_raw = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info(pc, 'ORDER')\n",
    "dataSource = dsx_core_utils.get_data_source_info(pc, dataSet['datasource'])\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = (dataSet['schema'] + '.' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table']\n",
    "order_raw = sparkSession.read.format(\"jdbc\").option(\"driver\",dataSource['driver_class']).option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dsx_core_utils.decrypt(dataSource['password'])).load()\n",
    "\n",
    "order = order_raw.toPandas()\n",
    "\n",
    "# This code rearranges the dataframe by dropping some columns, filling blanks and empty records,\n",
    "# renaming some other columns and resetting the index column.\n",
    "# Renaming some columns has been done for the sake of clarity in English. The names have been translated\n",
    "# from Czech to English\n",
    "\n",
    "order.drop([\"bank_to\", \"account_to\", \"order_id\"], axis=1, inplace=True)\n",
    "order.k_symbol.fillna(\"No_symbol\")\n",
    "order.k_symbol = order.k_symbol.str.replace(\" \", \"No_symbol\")\n",
    "order = order.groupby([\"account_id\", \"k_symbol\"]).mean().unstack()\n",
    "order = order.fillna(0)\n",
    "order.columns = order.columns.droplevel()\n",
    "order.reset_index(level=\"account_id\", col_level=1, inplace=True)\n",
    "order.rename_axis(\"\", axis=\"columns\", inplace=True)\n",
    "order.rename(\n",
    "    index=None,\n",
    "    columns={\n",
    "        \"LEASING\": \"order_amount_LEASING_PAYMENT\",\n",
    "        \"No_symbol\": \"order_amount_No_symbol\",\n",
    "        \"POJISTNE\": \"order_amount_INSURANCE_PAYMENT\",\n",
    "        \"SIPO\": \"order_amount_HOUSEHOLD_PAYMENT\",\n",
    "        \"UVER\": \"order_amount_LOAN_PAYMENT\",\n",
    "    },\n",
    "    inplace=True)\n",
    "\n",
    "order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 9\n",
    "\n",
    "# Load table LOAN\n",
    "\n",
    "# Add asset from remote connection\n",
    "loan_raw = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info(pc, 'LOAN')\n",
    "dataSource = dsx_core_utils.get_data_source_info(pc, dataSet['datasource'])\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = (dataSet['schema'] + '.' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table']\n",
    "loan_raw = sparkSession.read.format(\"jdbc\").option(\"driver\",dataSource['driver_class']).option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dsx_core_utils.decrypt(dataSource['password'])).load()\n",
    "\n",
    "# Date formatting using lambda function as before.\n",
    "\n",
    "loan = loan_raw.toPandas()\n",
    "loan.date = loan.date.apply(lambda x: pd.to_datetime(str(x), format=\"%y%m%d\"))\n",
    "loan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 10\n",
    "\n",
    "loan.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 11\n",
    "\n",
    "# Load table TRANS\n",
    "\n",
    "# Add asset from remote connection\n",
    "trans_raw = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info(pc, 'TRANS')\n",
    "dataSource = dsx_core_utils.get_data_source_info(pc, dataSet['datasource'])\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = (dataSet['schema'] + '.' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table']\n",
    "trans_raw = sparkSession.read.format(\"jdbc\").option(\"driver\",dataSource['driver_class']).option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dsx_core_utils.decrypt(dataSource['password'])).load()\n",
    "\n",
    "trans = trans_raw.toPandas()\n",
    "\n",
    "# Merge this table with LOAN after filling all empty records\n",
    "# Format date column\n",
    "\n",
    "trans.loc[trans.k_symbol == \"\", \"k_symbol\"] = trans[trans.k_symbol == \"\"].k_symbol.apply(lambda x: \"k_symbol_missing\")\n",
    "trans.loc[trans.k_symbol == \" \", \"k_symbol\"] = trans[trans.k_symbol == \" \"].k_symbol.apply(lambda x: \"k_symbol_missing\")\n",
    "loan_account_id = loan.loc[:, [\"account_id\"]]\n",
    "trans = loan_account_id.merge(trans, how=\"left\", on=\"account_id\")\n",
    "trans.date = trans.date.apply(lambda x: pd.to_datetime(str(x), format=\"%y%m%d\"))\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded all files with data and made the initial arrangements we start \"changing\" data into something understandable by the models: creating \"features\" for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3cb0e2516138e84f20c466fafc2d3a3bec3a91f1",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 12\n",
    "\n",
    "# create temp table trans_pv_k_symbol\n",
    "# Table is pivoted \n",
    "\n",
    "trans_pv_k_symbol = trans.pivot_table(values=[\"amount\", \"balance\"], index=[\"trans_id\"], columns=\"k_symbol\")\n",
    "trans_pv_k_symbol.fillna(0, inplace=True)\n",
    "trans_pv_k_symbol.columns = [\"_\".join(col) for col in trans_pv_k_symbol.columns]\n",
    "trans_pv_k_symbol = trans_pv_k_symbol.reset_index()\n",
    "trans_pv_k_symbol = trans.iloc[:, :3].merge(trans_pv_k_symbol, how=\"left\", on=\"trans_id\")\n",
    "trans_pv_k_symbol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b5c5c806bfdb7d043d8095091afb758e152581c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 13\n",
    "\n",
    "# create temp table get_date_loan_trans\n",
    "# First merge loan and account\n",
    "\n",
    "get_date_loan_trans = pd.merge(\n",
    "    loan,\n",
    "    account,\n",
    "    how=\"left\",\n",
    "    on=\"account_id\",\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_loan\", \"_account\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None)\n",
    "\n",
    "# Then merge the former with loan-account with trans. All of them use account_id as the column guiding the merging.\n",
    "\n",
    "get_date_loan_trans = pd.merge(\n",
    "    get_date_loan_trans,\n",
    "    trans,\n",
    "    how=\"left\",\n",
    "    on=\"account_id\",\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_account\", \"_trans\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None)\n",
    "\n",
    "#### Insert here the head() method to view how merging has configured the new table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f870251813963c0a42973806f950f904be63390",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 14\n",
    "\n",
    "# update table get_date_loan_trans to get the date between loan_date and trans_date. Then format it to leave a plain number of days.\n",
    "\n",
    "get_date_loan_trans[\"date_loan_trans\"] =  #### Insert code before this comment ####\n",
    "get_date_loan_trans[[\"date_loan_trans\"]] = get_date_loan_trans[[\"date_loan_trans\"]].astype(str)\n",
    "\n",
    "# Format the new column to set the number of days as a number\n",
    "get_date_loan_trans.date_loan_trans = get_date_loan_trans.date_loan_trans.str.strip(\" days 00:00:00.000000000\")\n",
    "get_date_loan_trans.date_loan_trans = pd.to_numeric(get_date_loan_trans.date_loan_trans.str.strip(\" days +\"))\n",
    "get_date_loan_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05b3e5f19e00bb49d8c1d29bfcf86afcb8f83ec1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 15\n",
    "\n",
    "# create temp table temp_90_mean to create new feature\n",
    "temp_90_mean = get_date_loan_trans[(get_date_loan_trans[\"date_loan_trans\"] >= 0) & (get_date_loan_trans[\"date_loan_trans\"] < 90)]\n",
    "temp_90_mean = temp_90_mean.drop([\"trans_id\", \"k_symbol\"], axis=1)\n",
    "temp_90_mean = temp_90_mean.groupby([\"loan_id\"], as_index=None).mean()\n",
    "temp_90_mean = temp_90_mean.loc[:, [\"loan_id\", \"balance\"]]\n",
    "temp_90_mean.rename(index=None, columns={\"balance\": \"avg_balance_3M_before_loan\"}, inplace=True)\n",
    "\n",
    "#### Insert code to view the 7 last lines of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ddcc6c7f2a499d28b152becb71c7debab5efd69",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 16\n",
    "\n",
    "# create temp table temp_30_mean to create new feature\n",
    "temp_30_mean = get_date_loan_trans[(get_date_loan_trans[\"date_loan_trans\"] >= 0) & (get_date_loan_trans[\"date_loan_trans\"] < 30)]\n",
    "temp_30_mean = temp_30_mean.drop([\"trans_id\", \"k_symbol\"], axis=1)\n",
    "temp_30_mean = temp_30_mean.groupby([\"loan_id\"], as_index=None).mean()\n",
    "temp_30_mean = temp_30_mean.loc[:, [\"loan_id\", \"balance\"]]\n",
    "temp_30_mean.rename(index=None, columns={\"balance\": \"avg_balance_1M_before_loan\"}, inplace=True)\n",
    "\n",
    "#### Insert code to view the 7 last lines of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b4df0d9ad816de2e13075bcbd24517806e97aa2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 17\n",
    "\n",
    "# create temp table temp_trans_freq to create new feature.\n",
    "# A frequency of movements is created\n",
    "temp_before = get_date_loan_trans[(get_date_loan_trans[\"date_loan_trans\"] >= 0)]\n",
    "temp_trans_freq = (temp_before.loc[:, [\"loan_id\", \"trans_id\"]].groupby([\"loan_id\"], as_index=None).count())\n",
    "temp_trans_freq.rename(index=None, columns={\"trans_id\": \"trans_freq\"}, inplace=True)\n",
    "temp_before = temp_before.drop([\"trans_id\", \"k_symbol\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d11635d9b0e29684c7f3b22842cded5ff3b02ae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 18\n",
    "\n",
    "# create temp table temp_balance_min & temp_balance_mean to create new features\n",
    "# Minimun and average balances are added for each of the loans\n",
    "\n",
    "temp_balance_min = (\n",
    "    temp_before.groupby([\"loan_id\"], as_index=None).min().loc[:, [\"loan_id\", \"balance\"]]\n",
    ")\n",
    "temp_balance_min.rename(\n",
    "    index=None, columns={\"balance\": \"min_balance_before_loan\"}, inplace=True\n",
    ")\n",
    "\n",
    "temp_balance_mean = (\n",
    "    temp_before.groupby([\"loan_id\"], as_index=None)\n",
    "    .mean()\n",
    "    .loc[:, [\"loan_id\", \"amount_trans\", \"balance\"]]\n",
    ")\n",
    "temp_balance_mean.rename(\n",
    "    index=None,\n",
    "    columns={\n",
    "        \"amount_trans\": \"avg_amount_trans_before_loan\",\n",
    "        \"balance\": \"avg_balance_before_loan\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "temp_balance_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9dd7ad8fcdadade8e4f5306840254cff0475db3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 19\n",
    "\n",
    "# create temp table times_balance_below_500 & times_balance_below_5K to create new features\n",
    "\n",
    "times_balance_below_500 = temp_before[temp_before.balance < 500]\n",
    "times_balance_below_500 = (\n",
    "    times_balance_below_500.groupby([\"loan_id\"], as_index=None)\n",
    "    .count()\n",
    "    .loc[:, [\"loan_id\", \"balance\"]]\n",
    ")\n",
    "times_balance_below_500 = times_balance_below_500[times_balance_below_500.balance > 1]\n",
    "times_balance_below_500.rename(\n",
    "    index=str, columns={\"balance\": \"times_balance_below_500\"}, inplace=True\n",
    ")\n",
    "\n",
    "times_balance_below_5K = temp_before[temp_before.balance < 5000]\n",
    "times_balance_below_5K = (\n",
    "    times_balance_below_5K.groupby([\"loan_id\"], as_index=None)\n",
    "    .count()\n",
    "    .loc[:, [\"loan_id\", \"balance\"]]\n",
    ")\n",
    "times_balance_below_5K = times_balance_below_5K[times_balance_below_5K.balance > 1]\n",
    "times_balance_below_5K.rename(\n",
    "    index=str, columns={\"balance\": \"times_balance_below_5K\"}, inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e31d74020620cb91f349e2935c72d65509da178f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 20\n",
    "\n",
    "# create temp table merge_loan_trans to merge the temp features above into one temp table\n",
    "merge_loan_trans = loan.merge(\n",
    "    temp_90_mean, how=\"left\", on=\"loan_id\", suffixes=(\"_loan\", \"_trans\")\n",
    ")\n",
    "merge_loan_trans = merge_loan_trans.merge(temp_30_mean, how=\"left\", on=\"loan_id\")\n",
    "merge_loan_trans = merge_loan_trans.merge(temp_trans_freq, how=\"left\", on=\"loan_id\")\n",
    "merge_loan_trans = merge_loan_trans.merge(temp_balance_min, how=\"left\", on=\"loan_id\")\n",
    "merge_loan_trans = merge_loan_trans.merge(temp_balance_mean, how=\"left\", on=\"loan_id\")\n",
    "merge_loan_trans = merge_loan_trans.merge(\n",
    "    times_balance_below_500, how=\"left\", on=\"loan_id\"\n",
    ")\n",
    "\n",
    "#### Insert code below to merge the table times_balance_below_5K into the table merge_loan_trans on the left and using loan_id as a guide\n",
    "\n",
    "\n",
    "\n",
    "#### Visualize the resulting dataframe using head() method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd3b22141d7af278e83c2afb5bfa46f95293f68a",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 21\n",
    "\n",
    "# | (pipe) operator is the union of sets when used with dataframes\n",
    "\n",
    "loan_BorD = loan[(loan.status == \"D\") | (loan.status == \"B\")]\n",
    "\n",
    "#### Insert code below. Need to know how maby rows are there after subsetting D and B status.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2784ef9eba83737518466a93a86fd11bf0362a2",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 22\n",
    "\n",
    "temp = times_balance_below_500.merge(\n",
    "    loan,\n",
    "    how=\"inner\",\n",
    "    on=\"loan_id\",\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None)\n",
    "\n",
    "#### Insert code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f63ada06adc028aeb1208b0bc8780d054a0cc980",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 23\n",
    "\n",
    "#### Insert code below to plot status vs times_balance_below_500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e9ba8b52a3df965c65164b9500c61571be333ff",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 24\n",
    "\n",
    "# We get different views to try to understand the behaviour of payments and balances\n",
    "\n",
    "temp.sort_values(\"times_balance_below_500\", ascending=False).plot(x=\"status\", y=\"times_balance_below_500\", kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8404c88bdc54b3590ebc98eb8830b901c738361a",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 25\n",
    "\n",
    "#### Insert code below. Select all rows but just two columns: payments and status. Check that it's correct viewing the 3 first rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef77c5267f9ecb7a6706e929c985b7d7b6e70411",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 26\n",
    "\n",
    "# Group the 4 status and take a mean of the payments made in each of the four groups\n",
    "\n",
    "t = t.groupby([\"status\"], as_index=None).mean()\n",
    "\n",
    "#### Insert code below. Plot the former calculation with status in x axis \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b731fa579a68ff58757848461bc52095e05c6f24"
   },
   "source": [
    "# Merge tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are going to create a version of the dataframe that will be used in LAB-3 to train our machine learning models. To do this we are merging some of the original tables, loaded from Db2 for z/OS (with the arrangements done at load time), with some of the dataframes created along this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1a4d110bc546a126f9109be5a80fda44b6e83d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 27\n",
    "\n",
    "# Merge the created merge_loan_trans with account. Have a look to the options used to merge.\n",
    "\n",
    "df = pd.merge(\n",
    "    merge_loan_trans,\n",
    "    account,\n",
    "    how=\"left\",\n",
    "    on=\"account_id\",\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_loan\", \"_account\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15583e9948f477794232b437677ad6f757f8cb1f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 28\n",
    "\n",
    "# Add order to the new dataframe df\n",
    "\n",
    "#### Insert code below. Merge the already created dataframe, df, with the dataframe for the ACCOUNT table, whose name is account too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cad7c0daa3527bc8ee10b7ef4605cf497897f747",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 29\n",
    "\n",
    "# Add disp\n",
    "\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    disp,\n",
    "    how=\"left\",\n",
    "    on=\"account_id\",\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_b\", \"_disp\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30cd61df1d5df945bedfa3a8a632ed31f3f229cd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 30\n",
    "\n",
    "# Add card\n",
    "\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    card,\n",
    "    how=\"left\",\n",
    "    on=\"disp_id\",\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_c\", \"_card\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59e3c6cccd3d5eeb8ccfa6febeb1c6b46778e64d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 31\n",
    "\n",
    "# Add client\n",
    "\n",
    "#### Insert code below. Merge df with client.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83cacef059f289183b2fa515cd6ce6d39b1b9cd5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 32\n",
    "\n",
    "# Add district\n",
    "\n",
    "#### Insert code below. Merge df with district.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f85cc33b1b65f23feceed68291314aec6a7139e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 33\n",
    "\n",
    "# Some arrangements before merging with df. We use before_loan_date and trans_pv_k_symbol after some selection from before_loan_date\n",
    "\n",
    "before_loan_date = get_date_loan_trans[(get_date_loan_trans[\"date_loan_trans\"] >= 0)]\n",
    "before_loan_date = before_loan_date.loc[:, [\"account_id\", \"trans_id\"]]\n",
    "trans_pv_k_symbol = pd.merge(\n",
    "    before_loan_date,\n",
    "    trans_pv_k_symbol,\n",
    "    how=\"left\",\n",
    "    on=\"trans_id\",\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_before\", \"_df2\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None)\n",
    "\n",
    "trans_pv_k_symbol.drop([\"account_id_df2\", \"date\", \"trans_id\"], axis=1, inplace=True)\n",
    "\n",
    "trans_pv_k_symbol.rename(columns={\"account_id_before\": \"account_id\"}, inplace=True)\n",
    "\n",
    "trans_pv_k_symbol = trans_pv_k_symbol.groupby(by=\"account_id\", axis=0, as_index=False, sort=True, group_keys=True, squeeze=False).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41464395b9ca5e12d2fb7a1cb6159db3da6fdcae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 34\n",
    "\n",
    "# And with this merge we have our final df\n",
    "\n",
    "#### Insert code below. Merge df with trans_pv_k_symbol\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee2a376881a04788b0adbd6b9eb9c224814cc554"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen before this final df dataframe needs some final cleaning and tyding up. Pay attention to the next group of cells and study what are these final changes affecting data. Feel free to insert code to check the status of the dataframe in any of the cells where manipulations are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d019cce0a19eb7ae897ae6047a6ed8334bf094f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 35\n",
    "\n",
    "df[\"year_\"] = df.date_loan.apply(lambda x: x.year, convert_dtype=int, args=())\n",
    "df[\"years_of_loan\"] = 1999 - df.year_\n",
    "df.drop([\"date_loan\", \"year_\"], axis=1, inplace=True)\n",
    "df.frequency = df.frequency.map(\n",
    "    {\"MONTHLY ISSUANCE\": 30, \"WEEKLY ISSUANCE\": 7, \"ISSUANCE AFTER TRANSACTION\": 1}\n",
    ")\n",
    "\n",
    "# Insert visualization code here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5308e19609f8d14deff98c183293a16e2ae4a6b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 36\n",
    "\n",
    "df[\"year_\"] = df.date_account.apply(lambda x: x.year, convert_dtype=int, args=())\n",
    "df[\"years_of_account\"] = 1999 - df.year_\n",
    "df.drop([\"date_account\", \"year_\", \"type_disp\"], axis=1, inplace=True)\n",
    "\n",
    "# Insert visualization code here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5a2bba278a2a1a4c93e3eec24d7b6e803348496",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 37\n",
    "\n",
    "df.issued.fillna(\"999999\", inplace=True)\n",
    "df[\"years_card_issued\"] = df.issued.apply(\n",
    "    lambda x: (99 - int(x[:2])), convert_dtype=int\n",
    ")\n",
    "df.drop([\"issued\",\"A12\",\"A15\"], axis=1, inplace=True)\n",
    "\n",
    "# Insert visualization code here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "035bc6035136c6aa8e3f1193e4f3b01e7d84ace8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 38\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Insert visualization code here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fa89334fde4b6744d651510f814482952b51885",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 39\n",
    "\n",
    "df.status.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55940ef9ba06afdab24baf2842b897daa2cd32c4",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 40\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "632eff5687a62d3efa53c2e3b014a264e44ad1f7"
   },
   "source": [
    "# Get Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca5b1056cc1f89ba7246cf97e228a5c11c3ef17e",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 41\n",
    "\n",
    "# Change categorical variables into numerical ones. There are ML model not admitting categoricals\n",
    "\n",
    "m = {\"A\": 0, \"B\": 1, \"C\": 0, \"D\": 1}\n",
    "df.status = df.status.map(m)\n",
    "\n",
    "#### Insert code below. Check that the unique values are 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ade11ae3b5be723a14a6893fb8220cb06a6559d2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 42\n",
    "\n",
    "df = pd.get_dummies(df, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b14fe83efd9145dcaa00d4117a146f0ca62c7c6d",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 43\n",
    "\n",
    "df.columns.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfd165c3a7fa6fcf95a2c9b2c7665bcf8012050b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 44\n",
    "\n",
    "#### Insert code below. Drop columns \n",
    "\n",
    "\n",
    "\n",
    "# Save the shining df dataframe into a CSV file to reuse it as needed\n",
    "\n",
    "df.to_csv('fullDataframe_EOLAB1_<login_userid>.csv', sep=';', encoding='utf-8',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We finally have a consistent dataframe which is suitable for model training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5510fe0fc9f1ff76098d70546917ebf3f907bff0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 45\n",
    "\n",
    "pwd"
   ]
  }
 ],
 "metadata": {
  "component": "ze6njb33",
  "kernelspec": {
   "display_name": "imlpython3",
   "language": "python",
   "name": "imlpython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
