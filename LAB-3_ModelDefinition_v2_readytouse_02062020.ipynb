{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78924bf39d16e0f9a8a1dcffbb0e9fb86151d98f"
   },
   "source": [
    "# LAB3 - Model Definitions and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the process of data manipulation we did in LAB2, we are ready to use our dataframe to train and set up machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split in two parts what otherwise would be a full notebook (LAB2 and LAB3). We have tried to reflect in two different labs the logical sctructure of working with data in a real data science experiment. This obliges us to add some extra code at the beginning of this lab. \n",
    "\n",
    "As we did in LAB-2, we need to do an initial set up of the notebook to have all the data available with the same structure we coded before.\n",
    "\n",
    "We first recreate the `import` sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.ticker as mtick\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import dsx_core_utils, requests, os, io\n",
    "from dsx_core_utils import ProjectContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reload some data that is required for this lab that was already formatted in LAB-2. We loaded it from a Db2 table in LAB-2 but we are now loading it from a CSV file, just for informational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 2\n",
    "\n",
    "#### Insert code below. Load LOAN table from Db2 as you did in LAB2.\n",
    "\n",
    "\n",
    "#### Insert code from the lab script here to load convert loan_raw into loan with the required options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load the dataframe built in LAB-2 with exactly the same structure created then. If you remember what you will remember that, by the end of LAB-2, you saved the formatted pandas dataframe in a CSV file. It is that file which we are loading now to continue working in the same conditions created in LAB-2.\n",
    "\n",
    "Let's check the working path for this environment in the USS of the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 3\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 4\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'fullDataframe_EOLAB1_<login_userid>.csv',\n",
    "    sep=\";\",\n",
    "    delimiter=None,\n",
    "    header=\"infer\",\n",
    "    names=None,\n",
    "    low_memory=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will start working on the machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard processing and Training/Test set Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to have a set of data used to train machine learning models. We are splitting our dataframe into two parts and will use one of them for training. The other part will be used to evaluate how models work after being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67b88076af7cc4d638b612e0d2530f4341362d46",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 5\n",
    "\n",
    "# Select status column as the y (our target) and all the rest as X (features) \n",
    "\n",
    "X = df.loc[:, df.columns != \"status\"]\n",
    "y = df.loc[:, \"status\"]\n",
    "\n",
    "#### Insert code below. Split the dataframe in 2 sets. 3% of the data will be used for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting we have several dataframes ready to use with the models. We have `X` containing the full list of features and `y` containing the targets. Remember, we want to know whether a new loaner has a risk of defaulting or not and that is reflected within the `status`. We also have `X_train` and `X_test` which are used for both stages: train and evaluate of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e11c9fe499221c69494d01c5f4afc3f161aa219"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cae9f13d9c5679da7c79f9de40325465c8a08781"
   },
   "source": [
    "# Draft Modeling: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running an initial Random Forest model because we need to build some small dataframes around it. Some initial visualizations will be helpful to understand the behaviour of the features and the influence they have on the overall fitting of the model. After resolving the influence of each of the features we will be able to build a more accurate model. That is the reason why this is a _draft model_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7b9ce9be5b9dfe4b9ac9a30b8a53d76deabfcf1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 6\n",
    "\n",
    "# This is how a model is trained. Nothing strange: just invoke the algorithm with the necessary parameters. If you want more information\n",
    "# you may visit the documentation centre: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "rf = ensemble.RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=1,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None\n",
    ")\n",
    "\n",
    "#### Insert code below. Train the model created above and get a target prediction. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb7cd23fe9829b18b82213305255d311504eefd0",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 7\n",
    "\n",
    "# For an explanation of the classification report and the following confusion matrix\n",
    "# Follow this link: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "\n",
    "#### Insert code below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca1e2af8e34f38deef70d195f366aff01b955cf3",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 8\n",
    "\n",
    "# A confusion matrix is a visual to help you understand how good predictions (y_pred) are for X_train compared to the test.\n",
    "cm1 = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "sns.heatmap(cm1,annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26b1301088128391f0da5e421e15a9b90195515a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 9\n",
    "\n",
    "# Check how features have an \"importance\" assigned. It is a measure of the influence of each in the classification process\n",
    "\n",
    "fi = rf.feature_importances_\n",
    "\n",
    "#### Inset code below. Print the array of importances. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5941dff4698469ba639cb003a42a28e9f7191f00",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 10\n",
    "\n",
    "#### Insert code below. Subset the dataframe to check the behaviour of importances and features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e4f848cafd98f50ff3ca4838f24b4bbdb34b337",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 11\n",
    "\n",
    "# Study the subset created in previous cell. Using a visual aid is very useful.\n",
    "# Plot features in x axis and importance in y axis\n",
    "# Limit the visual to 20 first features in importance\n",
    "\n",
    "importance = pd.DataFrame(\n",
    "    {\"feature\": feature_cols[:], \"importance\": rf.feature_importances_[:]})\n",
    "\n",
    "importance.sort_values(\n",
    "    by=\"importance\",\n",
    "    axis=0,\n",
    "    ascending=False,\n",
    "    inplace=True,\n",
    "    kind=\"quicksort\",\n",
    "    na_position=\"last\",\n",
    ")\n",
    "\n",
    "#### Insert code below to do the plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "583e0cd8d5f18deb4cc20d6c5ae79e7e3c24ff82"
   },
   "source": [
    "# Visualization & Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enter into a section to understand the behaviour of featured and the influence they have in the model. We will select the most relevant to try to improve model accuracy. After this process we will set up new models to have a comparison and choose the best fitting one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe2805514058eb04e9b6c526e7c4fd13c308ba6a",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 12\n",
    "\n",
    "# Get an idea of the distribution of defaults by sex. \n",
    "# Remember: sex 0 = women, sex 1 = men. Status 0 = all ok, Status 1 = default\n",
    "\n",
    "df.groupby([\"sex\", \"status\"])[\"status\"].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "086a706e391b212443fd91610b1bce22d6360adf",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 13\n",
    "\n",
    "# Visualize same result as in former cell using a stacked bar graph\n",
    "\n",
    "df.groupby([\"sex\", \"status\"])[\"status\"].size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()\n",
    ").unstack().plot(kind=\"bar\", stacked=True)\n",
    "\n",
    "#### Insert code below. Customize the graph to make it more understandable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fee7073c52606d0e35975909137b4e5ee12b9b17",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 14\n",
    "\n",
    "# Visualize age related to status\n",
    "\n",
    "#### Insert code below. Plot the relationship between age and status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "deda205617de3b738b902b6144b361fabb71e2b6",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 15\n",
    "\n",
    "# Plot years of having a card vs status\n",
    "\n",
    "#### Insert code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02ccff0df812412019abbfc83d1b768c6b984b8f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 16\n",
    "\n",
    "# Study the influence of age vs status. To do that you need to segment by ages\n",
    "# Define a function to create age bins\n",
    "\n",
    "# Binning:\n",
    "def binning(col, cut_points, labels=None):\n",
    "    # Define min and max values:\n",
    "    minval = col.min()\n",
    "    maxval = col.max()\n",
    "\n",
    "    # create list by adding min and max to cut_points\n",
    "    break_points = [minval] + cut_points + [maxval]\n",
    "\n",
    "    # if no labels provided, use default labels 0 ... (n-1)\n",
    "    if not labels:\n",
    "        labels = range(len(cut_points) + 1)\n",
    "\n",
    "    # Binning using cut function of pandas\n",
    "    colBin = pd.cut(col, bins=break_points, labels=labels, include_lowest=True)\n",
    "    return colBin\n",
    "\n",
    "\n",
    "# Binning age:\n",
    "#### Insert code below. Apply function binning to a sample of ages between 20 and 50 years\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6198229ab69b04780fe618698104f72e90c4482d",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 17\n",
    "\n",
    "# Plot the recently created column age_bin vs status\n",
    "\n",
    "#### Insert code below. Create a plot grouping age_bin with status vs status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2225d8324ba59d1ff954f09a00179361be07e370",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 18\n",
    "\n",
    "# Have a look to the dataframe selecting all rows marked with a 1 status (any kind of default)\n",
    "\n",
    "#### Insert code below. Select all rows with status 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c58de4e04acac32ef4248d957d8a851603f2da93",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 19\n",
    "\n",
    "# Graph status vs payments mean to have an idea of the behaviour of clients defaulting and not defaulting.\n",
    "\n",
    "#### Insert code below. Group averaged payments with status and plot it in a bar graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15c87a9fb1c91c14b51ae0edb04dfaf2e80cd08a",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 20\n",
    "\n",
    "# Plot a heatmap to understand the correlation of the main features. Limit it to a 10x10 matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cols = list(importance.feature[:10]) # To add or remove features to the matrix change this number\n",
    "cols.insert(0, \"status\")\n",
    "corrcoef_map = np.corrcoef(df[cols].values.T)\n",
    "fig, ax = plt.subplots(figsize=(12, 12))  # Sample figsize in inches\n",
    "hm = sns.heatmap(\n",
    "    corrcoef_map,\n",
    "    cbar=True,\n",
    "    annot=True,\n",
    "    square=True,\n",
    "    fmt=\".2f\",\n",
    "    annot_kws={\"size\": 15},\n",
    "    yticklabels=cols,\n",
    "    xticklabels=cols,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "#Optional. Fix for this version of the package matplotlib. \n",
    "\n",
    "#### Insert code below to fix the view. This is a problem with the present version of matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d268a0bc53d06bd693de3bf065d0567b942cef0"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run several tests and visualizations to understand our dataframes. We are now in a position to train some machine learning models and decide which one is the best for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a96f3340a5cf2f5c24b51fe7b0222f111c624251",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 21\n",
    "\n",
    "# Train and test a Random Forest model\n",
    "# Remember that in a previous cell we defined the dataframes like this:\n",
    "#X = df.loc[:, df.columns != \"status\"]\n",
    "#y = df.loc[:, \"status\"] \n",
    "\n",
    "rf = ensemble.RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=1,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None\n",
    ")\n",
    "\n",
    "#### Insert code below. Run the fitting and prediction of the model created above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82650f0fae785a965f132f2ac750cda02b9b9394",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 22\n",
    "\n",
    "# Get the classification report\n",
    "\n",
    "#### Insert code below. Print the classification report for this fit of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a5572007e6a1e92f3350c2481cb3eae76bd5f2f",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 23\n",
    "\n",
    "# Get the confusion matrix for the model\n",
    "\n",
    "#### Insert code below. Print the confusion matrix for this fit of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "929a973a96b8af1aba5d91b8fa04e844d1abf9ec",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 24\n",
    "\n",
    "# Build feature and importance dataframe to further check the behaviour of the model\n",
    "\n",
    "feature_cols = X_test.columns\n",
    "importance = pd.DataFrame(\n",
    "    {\"feature\": feature_cols[:], \"importance\": rf.feature_importances_[:]}\n",
    ")\n",
    "\n",
    "importance.sort_values(\n",
    "    by=\"importance\",\n",
    "    axis=0,\n",
    "    ascending=False,\n",
    "    inplace=True,\n",
    "    kind=\"quicksort\",\n",
    "    na_position=\"last\"\n",
    ")\n",
    "\n",
    "\n",
    "importance[:18].plot(x=\"feature\", y=\"importance\", kind=\"bar\")\n",
    "plt.ylabel(\"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 25\n",
    "\n",
    "# Now you will export this model to the repository on z/OS so that it will be available for use in test and production\n",
    "\n",
    "from repository_v3.mlrepository import MetaNames\n",
    "from repository_v3.mlrepository import MetaProps\n",
    "from repository_v3.mlrepositoryclient import MLRepositoryClient\n",
    "from repository_v3.mlrepositoryartifact import MLRepositoryArtifact\n",
    "import pprint\n",
    "\n",
    "metaservicePath = \"https://10.3.72.69:443\"\n",
    "client = MLRepositoryClient(metaservicePath)\n",
    "client.authorize_with_token(pc.authToken)\n",
    "\n",
    "props1 = MetaProps(\n",
    "        {MetaNames.AUTHOR_NAME:\"author\",\n",
    "         MetaNames.AUTHOR_EMAIL:\"author@example.com\",\n",
    "         MetaNames.MODEL_META_PROJECT_ID: pc.projectName,\n",
    "         MetaNames.MODEL_META_ORIGIN_TYPE: \"notebook\",\n",
    "         MetaNames.SCOPE: \"Project\",\n",
    "         MetaNames.MODEL_META_ORIGIN_ID: pc.nbName})\n",
    "\n",
    "input_artifact = MLRepositoryArtifact(rf, \n",
    "      name=\"loan_random_forest\", meta_props=props1, # Change the name of the model if needed\n",
    "      training_data=X_train, training_target=y_train)\n",
    "\n",
    "client.models.save(artifact=input_artifact)\n",
    "print(\"model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0aa4fd0274f08d13b6cb646f78b738e0e31377ba"
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning purposes we are creating and training some additional machine learning models. This allows us to compare the how different models give different results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd3379a3fcee765713323c19e0086ac5f661fc69",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 26\n",
    "\n",
    "# Create and train a decision tree model\n",
    "\n",
    "#### Insert code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70af723776dd4797c056d2fd3285e17cdcaaa305",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 27\n",
    "\n",
    "# Get the classification report\n",
    "\n",
    "#### Insert code below. Print the classification report for this fit of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d62b95f5a3895e56c768a02c908761e833038062",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 28\n",
    "\n",
    "# Plot the confusion matrix for this decision tree\n",
    "\n",
    "#### Insert code below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "739f89fb8118d9ddaf2e1bfbcc47416cfb95cb58",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 29\n",
    "\n",
    "# Classify features by order of importance for the decision tree\n",
    "\n",
    "feature_cols = X_test.columns\n",
    "importance = pd.DataFrame(\n",
    "    {\"feature\": feature_cols[:], \"importance\": rf.feature_importances_[:]}\n",
    ")\n",
    "importance.sort_values(\n",
    "    by=\"importance\",\n",
    "    axis=0,\n",
    "    ascending=False,\n",
    "    inplace=True,\n",
    "    kind=\"quicksort\",\n",
    "    na_position=\"last\",\n",
    ")\n",
    "importance[:18].plot(x=\"feature\", y=\"importance\", kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 30\n",
    "\n",
    "# Copy the code used to export the previous model and update this cell. It is in CELL 25\n",
    "# Delete the imports which have already done in previous cell \n",
    "# Update the name of the model before executing the cell \n",
    "\n",
    "### Insert code below. Export decision tree model to the repository.\n",
    "from repository_v3.mlrepository import MetaNames\n",
    "from repository_v3.mlrepository import MetaProps\n",
    "from repository_v3.mlrepositoryclient import MLRepositoryClient\n",
    "from repository_v3.mlrepositoryartifact import MLRepositoryArtifact\n",
    "import pprint\n",
    "\n",
    "metaservicePath = \"https://10.3.72.69:443\"\n",
    "client = MLRepositoryClient(metaservicePath)\n",
    "client.authorize_with_token(pc.authToken)\n",
    "\n",
    "props1 = MetaProps(\n",
    "        {MetaNames.AUTHOR_NAME:\"author\",\n",
    "         MetaNames.AUTHOR_EMAIL:\"author@example.com\",\n",
    "         MetaNames.MODEL_META_PROJECT_ID: pc.projectName,\n",
    "         MetaNames.MODEL_META_ORIGIN_TYPE: \"notebook\",\n",
    "         MetaNames.SCOPE: \"Project\",\n",
    "         MetaNames.MODEL_META_ORIGIN_ID: pc.nbName})\n",
    "\n",
    "input_artifact = MLRepositoryArtifact(rf, \n",
    "      name=\"loan_random_forest\", meta_props=props1, # Change the name of the model if needed\n",
    "      training_data=X_train, training_target=y_train)\n",
    "\n",
    "client.models.save(artifact=input_artifact)\n",
    "print(\"model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4914522f6cfb28289f28a844b60d878bec6063c7"
   },
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing the same work as for previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "820d2c7d61fea293068de7ba6e267661e4cb8247",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 31\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create the model and set parameters\n",
    "\n",
    "gbc = GradientBoostingClassifier(\n",
    "    loss=\"deviance\",\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    subsample=1.0,\n",
    "    criterion=\"friedman_mse\",\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_depth=3,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    init=None,\n",
    "    random_state=None,\n",
    "    max_features=None,\n",
    ")\n",
    "\n",
    "# Train the model and fit data\n",
    "model = gbc.fit(X_train, y_train)\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Get the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21bcc5957a2b1289c34323de883ef40e4ce5ff85",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 32\n",
    "\n",
    "# Get the confusion matrix for this Gradient Boost Classifier\n",
    "\n",
    "#### Insert code below. Plot the confusion matrix for this GBC model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 33\n",
    "\n",
    "# Copy the code used to export the previous model and update this cell\n",
    "# Delete the imports which have already done in previous cell \n",
    "# Update the name of the model before executing the cell \n",
    "\n",
    "### Insert code below. Export GBC model to the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7cb1f51792ce5c49c7abb8d5c378a322fed3549"
   },
   "source": [
    "# Feature Scaling for SVM & Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start working with Support Vector Machine (SVM) algorithms we need to do some technical work calle _feature scaling_. You may learn a little bit more about this in this [Wikipedia entry](https://en.wikipedia.org/wiki/Feature_scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd6ed7b570129bd1973910ecd62eacc224b5b0fb",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 34\n",
    "\n",
    "# Standard processing for feature scaling\n",
    "\n",
    "sc = StandardScaler()\n",
    "X.drop(['age'], axis=1, inplace=True)\n",
    "X = sc.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccadff9e5e2e23d07aa4159c408acf179e2f79a1"
   },
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4025c9ad3e7169a68c67701ced30c0da512cf67",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 35\n",
    "\n",
    "# Create, train and fit the SVM model. Print the classification model\n",
    "\n",
    "svc = svm.SVC(\n",
    "    C=5,\n",
    "    kernel=\"rbf\",\n",
    "    degree=3,\n",
    "    gamma=\"auto\",\n",
    "    coef0=0.0,\n",
    "    shrinking=True,\n",
    "    probability=False,\n",
    "    tol=0.001,\n",
    "    cache_size=200,\n",
    "    class_weight=None,\n",
    "    verbose=False,\n",
    "    max_iter=-1,\n",
    "    decision_function_shape=\"ovr\",\n",
    "    random_state=None,\n",
    ")\n",
    "model = svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f38539aa3c748445c83740beca7af94ac571c885",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 36\n",
    "\n",
    "# Get the confusion matrix as with every other model.\n",
    "\n",
    "#### Insert code below. Get the confusion matrix for your SVM model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 37\n",
    "\n",
    "# Copy the code used to export the previous model and update this cell\n",
    "# Delete the imports which have already done in previous cell \n",
    "# Update the name of the model before executing the cell \n",
    "\n",
    "### Insert code below. Export SVM model to the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a08943ae895538f94c83a15894bb10a7e3ceb386"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e2783fd3f05e14ed3d7738751d4481e2c89ae1c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 38\n",
    "\n",
    "# Create and fit a Logistic Regression model\n",
    "\n",
    "#### Insert code below. Invoke the model and fit it, then make your predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "950996a2498819767be00bcbd9a1c6d316d84f97",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 39\n",
    "\n",
    "# Get the confusion matrix for the Logistic Regression model\n",
    "\n",
    "cm6 = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "sns.heatmap(cm6,annot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 40\n",
    "\n",
    "# Copy the code used to export the previous model and update this cell\n",
    "# Delete the imports which have already done in previous cell \n",
    "# Update the name of the model before executing the cell \n",
    "\n",
    "### Insert code below. Export logistic regression model to the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END OF LAB-3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd59bf2adddbfddb9a3d2e37551890696de0bc9d"
   },
   "source": [
    "# Try: Use only two features to plot Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are adding here some more cells for you to work on some additional areas of modelling. \n",
    "\n",
    "Some models like SVM and Logistic Regression. Decision boundaries are aimed at finding the boundaries between different features in a model. We are using just two features because it's easier to understand how to manage decision boundaries.\n",
    "\n",
    "With two features there will be just a boundary which will be a single line separating the data points into two regions. Sets of data points in each regions are called _classes_. 2 features = 2 classes.\n",
    "\n",
    "Read the comments inside the code for suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a532e7a5d3160eff52f4a7f33d23371986049e90",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 41\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha=0.6)\n",
    "    ax.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, alpha=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ed55b49fa7262c22a7ba364b9f4d3361fccdfcb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 42\n",
    "\n",
    "# You can test here changing times_balance_below_5K for some other feature\n",
    "# the second best fitting, for example: X = df[[\"min_balance_before_loan\", \"amount_INTEREST IF NEG. BALANCE\"]]\n",
    "\n",
    "X = df[[\"min_balance_before_loan\", \"times_balance_below_5K\"]]\n",
    "y = df[\"status\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88b586c4b708de84da6fd4a1d5e266364be40f4a",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 43\n",
    "\n",
    "rf = ensemble.RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=4,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=1,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "f1 = f1_score(y_pred, y_test)\n",
    "f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50a34d9fc6f66f07a96f28317c41e98659982eba",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 44\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8d843a4bbbbb38cca191c6e8e93b71af725b0ff",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 45\n",
    "\n",
    "cm7 = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "sns.heatmap(cm7,annot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cd8037bdb968e454911ef512a2c119c22956998",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 46\n",
    "\n",
    "plot_decision_boundary(model, X_test, y_test)\n",
    "plt.xlabel(\"min_balance_before_loan\")\n",
    "\n",
    "# If you have changed the column times_balance_below_5K for some other,\n",
    "# You'll have to change the label in this graph.\n",
    "plt.ylabel(\"times_balance_below_5K\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "058bee8d5171de84d4be924aa04676452839ac03",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 47\n",
    "\n",
    "# This kind of graph is easy to understand with two features, \n",
    "# if more features are used we might not be able to graph it\n",
    "# because it would exceed 3 dimensions.\n",
    "\n",
    "feature_cols = X_test.columns\n",
    "importance = pd.DataFrame(\n",
    "    {\"feature\": feature_cols[:], \"importance\": rf.feature_importances_[:]}\n",
    ")\n",
    "importance.sort_values(\n",
    "    by=\"importance\",\n",
    "    axis=0,\n",
    "    ascending=False,\n",
    "    inplace=True,\n",
    "    kind=\"quicksort\",\n",
    "    na_position=\"last\",\n",
    ")\n",
    "importance[:18].plot(x=\"feature\", y=\"importance\", kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5510fe0fc9f1ff76098d70546917ebf3f907bff0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "component": "ze1g016d",
  "kernelspec": {
   "display_name": "imlpython3",
   "language": "python",
   "name": "imlpython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
